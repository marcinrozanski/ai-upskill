services:
  qdrant:
    image: qdrant/qdrant                 # current stable build
    restart: unless-stopped
    ports: [ "6333:6333" ]
    volumes:
      - ./qdrant_storage:/qdrant/storage

  llama:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llama-cpu
    command: >
      -m /models/Phi-3-mini-4k-instruct-q4.gguf
      --port 8000
      --threads 8
      --ctx-size 4096
    volumes:
      - ./models:/models
    ports: [ "8001:8000" ]
    restart: unless-stopped

  api:
    build: ./app
    depends_on: [ qdrant, llama ]
    environment:
      - QDRANT_URL=http://qdrant:6333
      - LLM_URL=http://llama:8000/v1/chat/completions
    ports: [ "8000:8000" ]
    restart: unless-stopped
